{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP Clase 5 - Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### TP Clase 5: Competición de CNN!\n",
        "\n",
        "    Vamos a armar una pequeña competición en el curso.\n",
        "    El objetivo es armar una arquitectura de CNN que identifique el dataset MNIST.\n",
        "    Se van a usar capas de convolución, de activación y de pooling a elección. Cada alumno eligirá su modelo y los respectivos hiperparámetros, lo entrenará y presentará los siguientes resultados:\n",
        "\n",
        "    *   `test_acc` (del test final)\n",
        "    *   `n_parameter`\n",
        "    *   `n_layers` (conv + activacion + pooling = 1 capa)\n",
        "    *   `n_epochs` de entrenamiento usadas.\n",
        "\n",
        "    El modelo se deberá ajustar a los siguientes puntos:\n",
        "\n",
        "    *   train: 80%, validation: 10%, test: 10% (los datos serán dados para que todos usan el mismo set para cada grupo. Están en el github el curso).\n",
        "    *   capa final de salida será una softmax de 10 elementos.\n",
        "    *   cost_function será `CrossEntropyLoss`.\n",
        "\n",
        "    El ganador de la competencia será aquel que consiga el mayor `score` empleando la siguietne fórmula:\n",
        " \n",
        " ![equation](https://latex.codecogs.com/svg.image?score%20=%20\\frac{1}{log_{10}(n\\_parameter)}%20*%20\\frac{10}{n\\_epochs}*test\\_acc*n\\_layers)\n",
        "  \n",
        "  Deberan presentar su código colab funcionando y el score alcanzado (con los valores de cada variable que compone el score).\n",
        "   \n",
        "   Se armará un ranking con el score alcanzado, la red ganadora se llavará el título del campeon!\n",
        "    \n",
        "   Es una competencia fairplay y con fines didácticos, esta formula del ```score``` fué inventada.... no usar como referencia para definir qué modelo utilizar."
      ],
      "metadata": {
        "id": "X1W7Luud5NnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OJ9I-IWD5QnX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos kernell \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "rj5Ra5fDA8z2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos\n",
        "import pickle\n",
        "\n",
        "X_test = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )), (7000, 1, 28, 28)))\n",
        "y_test = torch.Tensor(np.array(pickle.load( open( \"/content/test_label.pkl\", \"rb\" ) )))\n",
        "y_test = y_test.type(torch.LongTensor)\n",
        "X_train = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/train.pkl\", \"rb\" ) )), (56000, 1, 28, 28)))\n",
        "y_train = torch.Tensor(np.array(pickle.load( open( \"/content/train_label.pkl\", \"rb\" ))) )\n",
        "y_train = y_train.type(torch.LongTensor)\n",
        "X_val = torch.Tensor(np.resize(np.array(pickle.load( open( \"/content/val.pkl\", \"rb\" ) )), (7000, 1, 28, 28)))\n",
        "y_val = torch.Tensor(np.array(pickle.load( open( \"/content/val_label.pkl\", \"rb\" ))) )\n",
        "y_val = y_val.type(torch.LongTensor)\n",
        "print('X_train: ',X_train.shape, ' y_train: ',y_train.shape)\n",
        "print('X_test: ',X_test.shape, ' y_test: ',y_test.shape)\n",
        "print('X_val: ',X_val.shape, ' y_val: ',y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoyH9gyxDaB4",
        "outputId": "ce3a44d5-272a-455c-d38b-9e2e3c27d953"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:  torch.Size([56000, 1, 28, 28])  y_train:  torch.Size([56000])\n",
            "X_test:  torch.Size([7000, 1, 28, 28])  y_test:  torch.Size([7000])\n",
            "X_val:  torch.Size([7000, 1, 28, 28])  y_val:  torch.Size([7000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos y los tranformamos en tensores\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "std = np.std(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )))\n",
        "mean = np.mean(np.array(pickle.load( open( \"/content/test.pkl\", \"rb\" ) )))\n",
        "\n",
        "transform=torchvision.transforms.Compose([torchvision.transforms.Normalize(mean,std)])\n",
        "X_train_t = transform(X_train)\n",
        "X_test_t = transform(X_test)\n",
        "X_val_t = transform(X_val)\n",
        "train_dataset = TensorDataset(X_train_t,y_train) \n",
        "test_dataset = TensorDataset(X_test_t,y_test) \n",
        "val_dataset = TensorDataset(X_val_t,y_val) \n",
        "\n",
        "dataloader = {\n",
        "    'train': DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'test': DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True),\n",
        "    'val': DataLoader(val_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "}\n",
        "\n",
        "# Verificamos \n",
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PafXbfrXBCyj",
        "outputId": "34df3445-f4db-4c25-fcf6-8f5d0fb1b757"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label from dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(dataloader['train']))\n",
        "\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "img = train_features[0]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = train_features[0].squeeze()\n",
        "print('tamaño 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "oYUGrU9sFO0l",
        "outputId": "c796cad4-390f-4ccb-efce-ef42b3a5289c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n",
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTAMOS NUESTRA CNN"
      ],
      "metadata": {
        "id": "PyQSuvUNkJmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos  bloque de una capa CNN\n",
        "\n",
        "# (hiper)parámetros a pasar a la función:\n",
        "#   c_in:   canales (kernels) de entrada\n",
        "#   c_out:  canales (kernels) de salida\n",
        "#   k:      tamaño del kernel kxk\n",
        "#   p:      tamaño del padding de la convolución\n",
        "#   s:      stride de la convolución\n",
        "#   pk:     tamaño del kernel del pooling\n",
        "#   ps:     stride de la pooling\n",
        "#   pp:     padding en la pooling\n",
        "\n",
        "def block(c_in, c_out, k=3, p=2, s=1, pk=2, ps=2, pp=1):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.ReLU(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk, stride=ps, padding=pp))         # pooling"
      ],
      "metadata": {
        "id": "a8A1tTvekJBO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Armamos el objeto con nuestra red CNN\n",
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = block(n_channels, 64)\n",
        "    self.conv1_out = None\n",
        "    self.conv2 = block(64, 32)\n",
        "    self.conv2_out = None\n",
        "    self.conv3 = block(32, 32)\n",
        "    self.conv3_out = None\n",
        "    self.conv4 = block(32, 32)\n",
        "    self.conv4_out = None\n",
        "    self.conv5 = block(32, 32)\n",
        "    self.conv5_out = None\n",
        "    self.conv6 = block(32, 32)\n",
        "    self.conv6_out = None\n",
        "    self.conv7 = block(32, 32)\n",
        "    self.conv7_out = None\n",
        "    self.conv8 = block(32, 32)\n",
        "    self.conv8_out = None\n",
        "    self.conv9 = block(32, 32)\n",
        "    self.conv9_out = None\n",
        "    self.conv10 = block(32, 32)\n",
        "    self.conv10_out = None\n",
        "    self.conv11 = block(32, 32)\n",
        "    self.conv11_out = None\n",
        "    self.conv12 = block(32, 32)\n",
        "    self.conv12_out = None\n",
        "\n",
        "\n",
        "    \n",
        "    self.fc = torch.nn.Linear(512, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "    #self.sm = torch.nn.Softmax(dim=1)\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "    # Me fijo en el número de capas\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', i)\n",
        "    \n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', pytorch_total_params)\n",
        "\n",
        "  def validar_dim(self):\n",
        "    # es una funcion forward que imprime la dimension de cada paso\n",
        "    # la defino distinto de la forward standard para que cuando entrenemos\n",
        "    # no nos llene la pantalla de información inecesaria.\n",
        "\n",
        "    print(\"Validacion de dimensiones\")\n",
        "    tam = input(\"Ingrese tamaño de entrada: \")\n",
        "    x = torch.randn(1, 1, int(tam), int(tam))\n",
        "    print(\"Tamaño entrada: \", x.shape)\n",
        "    x = self.conv1(x)\n",
        "    print(\"Tamaño salida conv1: \", x.shape)\n",
        "    x = self.conv2(x)\n",
        "    #print(\"Tamaño salida conv2: \", x.shape)\n",
        "    x = self.conv3(x)\n",
        "    #print(\"Tamaño salida conv3: \", x.shape)\n",
        "    x = self.conv3(x)\n",
        "    #print(\"Tamaño salida conv3: \", x.shape)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    #print(\"Tamaño salida conv4: \", x.shape)\n",
        "    x = self.conv5(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.conv7(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.conv9(x)\n",
        "    x = self.conv10(x)\n",
        "    x = self.conv11(x)\n",
        "    x = self.conv12(x)\n",
        "\n",
        "\n",
        "\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    print(\"Tamaño imagen vectorizada: \", x.shape)\n",
        "    x = self.fc(x)\n",
        "    print(\"Tamaño salida fc (nro clases): \", x.shape)\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    return i, pytorch_total_params\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.conv1_out = self.conv1(x)\n",
        "    self.conv2_out = self.conv2(self.conv1_out)\n",
        "    self.conv3_out = self.conv3(self.conv2_out)\n",
        "    self.conv4_out = self.conv4(self.conv3_out)\n",
        "    self.conv5_out = self.conv5(self.conv4_out)\n",
        "    self.conv6_out = self.conv6(self.conv5_out)\n",
        "    self.conv7_out = self.conv7(self.conv6_out)\n",
        "    self.conv8_out = self.conv8(self.conv7_out)\n",
        "    self.conv9_out = self.conv9(self.conv8_out)\n",
        "    self.conv10_out = self.conv10(self.conv9_out)\n",
        "    self.conv11_out = self.conv11(self.conv10_out)\n",
        "    self.conv12_out = self.conv12(self.conv11_out)\n",
        "\n",
        "\n",
        "    y = self.conv12_out.view(self.conv12_out.shape[0], -1)\n",
        "    y = self.fc(y)\n",
        "    # x = self.sm(x)\n",
        "    return y"
      ],
      "metadata": {
        "id": "TxDekyWzCm6x"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la instancia del modelo\n",
        "\n",
        "model3 = CNN()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AtNW42u3gNo",
        "outputId": "f1fb2bce-0498-4b90-e853-8b877f5f149b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv6): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv7): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv8): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv9): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv10): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv11): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv12): Sequential(\n",
            "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  13\n",
            "Número total de parámetros a entrenar:  116714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observamos nuestro modelo y creamos nuestras variables que servirán para la métrica fina:\n",
        "n_layer, n_parameter = model3.validar_dim()\n",
        "print('cantidad de parametros entrenables:', n_parameter)\n",
        "print('cantidad de capas del modelo:', n_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQWKFJaD7AWd",
        "outputId": "8b688aca-4810-4ab0-d73a-1fbd02785998"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validacion de dimensiones\n",
            "Ingrese tamaño de entrada: 28\n",
            "Tamaño entrada:  torch.Size([1, 1, 28, 28])\n",
            "Tamaño salida conv1:  torch.Size([1, 64, 16, 16])\n",
            "Tamaño imagen vectorizada:  torch.Size([1, 512])\n",
            "Tamaño salida fc (nro clases):  torch.Size([1, 10])\n",
            "cantidad de parametros entrenables: 116714\n",
            "cantidad de capas del modelo: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la funcion de entrenamiento:\n",
        "\n",
        "from tqdm import tqdm # <- para graficar la barra de avance\n",
        "n_epochs = 1\n",
        "lr = 1e-3\n",
        "def fit(m, dataloader, epochs=n_epochs):\n",
        "    m.to(device)\n",
        "    optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        m.train()\n",
        "        train_loss, train_acc = [], []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = m(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            ####\n",
        "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "            train_acc.append(acc)\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
        "        bar = tqdm(dataloader['val'])\n",
        "        val_loss, val_acc = [], []\n",
        "        m.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = m(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
        "                val_acc.append(acc)\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
        "        "
      ],
      "metadata": {
        "id": "4bvvYmNV8i-N"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos la red:\n",
        "fit(model3, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYe-uVeX9OCE",
        "outputId": "200658e1-6ee8-4312-cbfb-0e5523fa973f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 0.82178 acc 0.70537: 100%|██████████| 875/875 [00:10<00:00, 81.52it/s]\n",
            "val_loss 0.17180 val_acc 0.95014: 100%|██████████| 110/110 [00:00<00:00, 152.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 loss 0.82178 val_loss 0.17180 acc 0.70537 val_acc 0.95014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "with torch.no_grad():\n",
        "    model3.eval()\n",
        "    X_test_t, y_test = X_test_t.to(device), y_test.to(device)\n",
        "    y_pred = model3(X_test_t)\n",
        "    test_loss = criterion(y_pred, y_test)\n",
        "    acc = (y_test == torch.argmax(y_pred, axis=1)).sum().item() / len(y_test)\n",
        "print('El Accuracy para el set de Test es de {} y la Loss es de {}'.format(acc,test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmxATAwA_3Hg",
        "outputId": "faa570f7-d14d-49bb-86df-a3aa29c2d61d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Accuracy para el set de Test es de 0.952 y la Loss es de 0.18120110034942627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora calculamos la métrica del concurso:\n",
        "n_layers = 12\n",
        "SCORE12 = (1 / np.log10(n_parameter)) * (10/n_epochs) * acc * n_layers\n",
        "print('EL SCORE ES DE', SCORE12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UKGKjwUI0Ut",
        "outputId": "43b0e6ef-0ce5-4ffd-cf3a-ebe84979cb9b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EL SCORE ES DE 22.545338064861145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSIONES: Como el peso de aumentar la cantidad de capas es mayor que el log10 de la cantidad de parámetros, utilizando 32 filtros para cada capa, lo mas conveniente es llevar al máximo la cantidad de capas.\n",
        "## Para ello utilicé la configuración de stride y padding (tanto en la capa de convolución como la de padding), que me permitan tener tantos layers como quiera porque converge en matrices de 4x4.\n",
        "## El problema es que llega un punto que al seguir aumentando las layers cae mucho el desempeño del modelo, debido a la gran cantidad de peso que tiene el padding en las capas finales."
      ],
      "metadata": {
        "id": "KsrukWMfrgdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dsc1TgGmsIWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}